{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"http://i.imgur.com/sSaOozN.png\" width=\"500\"></center>\n",
    "\n",
    "## Course: Computational Thinking for Governance Analytics\n",
    "\n",
    "### Prof. José Manuel Magallanes, PhD \n",
    "* Visiting Professor of Computational Policy at Evans School of Public Policy and Governance, and eScience Institute Senior Data Science Fellow, University of Washington.\n",
    "* Professor of Government and Political Methodology, Pontificia Universidad Católica del Perú. \n",
    "\n",
    "_____\n",
    "\n",
    "# Data Preprocessing in Python: Data Integration and Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will cover some important processes for DFs:\n",
    "* Appending\n",
    "* Reshaping\n",
    "* Merging\n",
    "\n",
    "Finally, we will see some **Exporting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Appending\n",
    "\n",
    "As the name implies, this process binds DFs into one, that is, one or more DFs will be put below or on top of another DF. Appending can be done when you fulfill these requisites:\n",
    "1. All the DFs  share the same column names.\n",
    "2. All the DFs  columns are in the same location.\n",
    "2. All the DFs  columns have the same data types.\n",
    "\n",
    "Let's visit this website: https://fundforpeace.org/what-we-do/country-risk-and-fragility-data/\n",
    "\n",
    "There, you will find several excel files with the _Fragile States Index_ per year. Let's downloan the ones for years 2019-2021 in the folder where this notebook is stored. Then, we can open them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file2021=\"fsi-2021.xlsx\"\n",
    "file2020=\"fsi-2020.xlsx\"\n",
    "file2019=\"fsi-2019.xlsx\"\n",
    "    \n",
    "\n",
    "# fetching the tables\n",
    "fragil2021=pd.read_excel(file2021)\n",
    "fragil2020=pd.read_excel(file2020)\n",
    "fragil2019=pd.read_excel(file2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to append all of them. \n",
    "\n",
    "In the next operations I will **sets**. Let me show you how they work with simple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=['a','b','c']\n",
    "B=['a','b','d','c']\n",
    "C=['a','b','e','g']\n",
    "\n",
    "# set intersection\n",
    "set(A) & set(B) & set(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, using set I can verify that condition 1 is fulfilled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C1: Security Apparatus',\n",
       " 'C2: Factionalized Elites',\n",
       " 'C3: Group Grievance',\n",
       " 'Country',\n",
       " 'E1: Economy',\n",
       " 'E2: Economic Inequality',\n",
       " 'E3: Human Flight and Brain Drain',\n",
       " 'P1: State Legitimacy',\n",
       " 'P2: Public Services',\n",
       " 'P3: Human Rights',\n",
       " 'Rank',\n",
       " 'S1: Demographic Pressures',\n",
       " 'S2: Refugees and IDPs',\n",
       " 'Total',\n",
       " 'X1: External Intervention',\n",
       " 'Year'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the columns shared among the DFs\n",
    "set(fragil2021.columns)&set(fragil2020)&set(fragil2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This intersection is offering me a particular order, then you can create DFs with columns located in that same order, which will allow us to fulfill the second condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sc/64t2qm0x53jbvl73k3glprw40000gn/T/ipykernel_47158/3266058975.py:4: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  fragil2021ap=fragil2021.loc[:,common]\n",
      "/var/folders/sc/64t2qm0x53jbvl73k3glprw40000gn/T/ipykernel_47158/3266058975.py:5: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  fragil2020ap=fragil2020.loc[:,common]\n",
      "/var/folders/sc/64t2qm0x53jbvl73k3glprw40000gn/T/ipykernel_47158/3266058975.py:6: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  fragil2019ap=fragil2019.loc[:,common]\n"
     ]
    }
   ],
   "source": [
    "common=set(fragil2021.columns)&set(fragil2020)&set(fragil2019)\n",
    "\n",
    "# we are keeping just the 'common' columns:\n",
    "fragil2021ap=fragil2021.loc[:,common]\n",
    "fragil2020ap=fragil2020.loc[:,common]\n",
    "fragil2019ap=fragil2019.loc[:,common]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last requisite is to make sure they share the same data types. Let's check the data types with the help of **zip**, **set** and **len**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'a', 'a'), ('b', 'b', 'b'), ('c', 'c', 'd')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=['a','b','c']\n",
    "B=['a','b','c']\n",
    "C=['a','b','d']\n",
    "\n",
    "# zip will pair elements in the same position:\n",
    "list(zip(A,B,C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining _len_ and _set_ you have a measure of 'variety'. If variety of a _set_ equals **1**, all the elements in a _set_ are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([1,1,1,1,1,1,1,1,1,1])), \\\n",
    "len(set([1,2,1])), \\\n",
    "len(set([1,111,11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you detect where a group does not have the same elements, that is, where the set size offers a variety greater than one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 'c', 'd')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if every zipped group has a variety greater than 1.\n",
    "[x for x in zip(A,B,C) if len(set(x))>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying that to our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(dtype('float64'), dtype('float64'), dtype('float64')),\n",
       " (dtype('O'), dtype('O'), dtype('O')),\n",
       " (dtype('float64'), dtype('float64'), dtype('float64')),\n",
       " (dtype('float64'), dtype('float64'), dtype('float64')),\n",
       " (dtype('float64'), dtype('float64'), dtype('float64')),\n",
       " (dtype('float64'), dtype('float64'), dtype('float64')),\n",
       " (dtype('float64'), dtype('float64'), dtype('float64')),\n",
       " (dtype('float64'), dtype('float64'), dtype('float64')),\n",
       " (dtype('O'), dtype('O'), dtype('O')),\n",
       " (dtype('int64'), dtype('<M8[ns]'), dtype('<M8[ns]')),\n",
       " (dtype('float64'), dtype('float64'), dtype('float64')),\n",
       " (dtype('float64'), dtype('float64'), dtype('float64')),\n",
       " (dtype('float64'), dtype('float64'), dtype('float64')),\n",
       " (dtype('float64'), dtype('float64'), dtype('float64')),\n",
       " (dtype('float64'), dtype('float64'), dtype('float64')),\n",
       " (dtype('float64'), dtype('float64'), dtype('float64'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. the zipped data types:\n",
    "theZips=zip(fragil2021ap.dtypes,\n",
    "            fragil2020ap.dtypes,\n",
    "            fragil2019ap.dtypes)\n",
    "# you see:\n",
    "list(theZips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(dtype('int64'), dtype('<M8[ns]'), dtype('<M8[ns]'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. computing the variety in each case above:\n",
    "theZips=zip(fragil2021ap.dtypes,\n",
    "            fragil2020ap.dtypes,\n",
    "            fragil2019ap.dtypes) # why again? zip is generator\n",
    "[x for x in theZips if len(set(x))>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, there is one set of columns that share different data types. Let me modify the previous code to detect the location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Year', dtype('int64'), dtype('<M8[ns]'), dtype('<M8[ns]'))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theZips=zip(fragil2021ap.dtypes.index,\n",
    "            fragil2021ap.dtypes,\n",
    "            fragil2020ap.dtypes,\n",
    "            fragil2019ap.dtypes)\n",
    "\n",
    "[x for x in theZips if len(set(x))>2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Year is not an integer in the DFs for 2020 and 2019, it is **date** type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2020-01-01\n",
       "1     2020-01-01\n",
       "2     2020-01-01\n",
       "3     2020-01-01\n",
       "4     2020-01-01\n",
       "         ...    \n",
       "173   2020-01-01\n",
       "174   2020-01-01\n",
       "175   2020-01-01\n",
       "176   2020-01-01\n",
       "177   2020-01-01\n",
       "Name: Year, Length: 178, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fragil2020ap['Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **dt** attribute of **date** column in Pandas, we can recover just the year as an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sc/64t2qm0x53jbvl73k3glprw40000gn/T/ipykernel_47158/743623927.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfragil2020ap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfragil2020ap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfragil2019ap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfragil2019ap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5573\u001b[0m         ):\n\u001b[1;32m   5574\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# https://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/accessors.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mPeriodProperties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .dt accessor with datetimelike values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "fragil2020ap['Year']=fragil2020ap['Year'].dt.year\n",
    "fragil2019ap['Year']=fragil2019ap['Year'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it should be fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theZips=zip(fragil2021ap.dtypes,fragil2020ap.dtypes,fragil2019ap.dtypes)\n",
    "[x for x in theZips if len(set(x))>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have met al the requisites, let's do the appending!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragile=pd.concat([fragil2021ap,fragil2020ap,fragil2019ap])\n",
    "fragile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the column ordering does not look nice, as in general you expect that the columns to the left start with identification of the rows rather than some measurements; then, let's move 'Country','Year','Total' to the left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a trick: setting columns as index\n",
    "fragile.set_index(['Country','Year','Total'],inplace=True)\n",
    "fragile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I will not use _Rank_, I will get rid of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragile.drop(columns='Rank',inplace=True)\n",
    "fragile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put the row indexes back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragile.reset_index(inplace=True)\n",
    "fragile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some cleaning on the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragile.columns=fragile.columns.str.replace(':|\\s',\"\",regex=True)\n",
    "fragile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's checl the format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragile.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reshaping\n",
    "\n",
    "Data frames have have different shapes. Let me keep some columns from the last DF so you can notice something:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragileLong=fragile.iloc[:,:3]\n",
    "fragileLong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit of analysis is country. Generally, we are used to see the  unit of analysis once in a column, but it is repeated above, as the country will appear for every year of measurement.\n",
    "\n",
    "Also, pay attention to the **amount of rows**. There are 535 rows, then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "535/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the amount of rows per data year, so something went wrong during the appending. The truth is that cleaning and formatting will be needed after complex operations like these.\n",
    "\n",
    "What is your best guess on what went wrong?\n",
    "\n",
    "Whatever it is, let me turn our **long** into **wide** shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide\n",
    "fragileWide=pd.pivot_table(fragileLong,\n",
    "               values='Total', # values to use\n",
    "               index=['Country'], # unit of analysis\n",
    "               columns=['Year']) # the values for NEW column\n",
    "fragileWide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **wide** shape from a **pivot_table** function looks great, but pay attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragileWide.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see above, the country is indeed and index, not a column. This is important if you are planing to export this DF. Also, notice that the column names have a title (_Year_). So in general, you can use this code after the *pivot_table* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragileWide= fragileWide.reset_index(drop=False).\\\n",
    "             rename_axis(index=None, columns=None)\n",
    "\n",
    "# result:\n",
    "\n",
    "fragileWide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mentioned that something went wrong during the appending process. Here, we can discover it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what cells have missing values?\n",
    "fragileWide[fragileWide.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even though the data DFs were prepared by the same [organization](https://fundforpeace.org/), the DFs have country names that differ among them. Here we need some **manual** changes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare changes as dict:\n",
    "changes={\"Cabo Verde\": \"Cape Verde\",\n",
    "\"Czechia\":\"Czech Republic\",\n",
    "\"Swaziland\":\"Eswatini\",\n",
    "\"Israel and West Bank\":\"Israel\",\n",
    "\"Kyrgyzstan\":\"Kyrgyz Republic\",\n",
    "\"North Macedonia\":\"Macedonia\",\n",
    "\"Slovakia\": \"Slovak Republic\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice I had to make the changes in the long shape of the DF, so that the wide shape will work fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make changes using 'replace':\n",
    "fragileLong.Country.replace(to_replace=changes,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redo the wide reshape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide\n",
    "fragileWide=pd.pivot_table(fragileLong,\n",
    "               values='Total',\n",
    "               index=['Country'],\n",
    "               columns=['Year']).\\\n",
    "            reset_index(drop=False).\\\n",
    "            rename_axis(index=None, columns=None)\n",
    "##\n",
    "fragileWide[fragileWide.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to be very careful when working with countries, specially when you are including or excluding countries; which may cause you hurting someone else's feelings. \n",
    "\n",
    "For instance, here I am just keeping **rows** with no missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragileWide.dropna(inplace=True) # axis=1 for columns\n",
    "fragileWide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, notice that the final line above says that the last row index is 178; which means there are 179 rows. To correct that, reset the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragileWide.reset_index(drop=True, inplace=True)\n",
    "fragileWide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sure, we can turn this wide format into a long one, using the function **melt**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(fragileWide, id_vars=['Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be more explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(fragileWide, #DF\n",
    "        id_vars=['Country'], #key\n",
    "        value_vars=[2019,2020,2021], # columns in wide\n",
    "        var_name='Year', # new name for long column\n",
    "        value_name='Total')# new name for values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging data sets need the following considerations:\n",
    "\n",
    "* Merging is done on two data frames.\n",
    "* You need a column in each data frame that share the same exact and unique values. The column names or titles need not be the same.\n",
    "* The merged table shows by default the mutual coincidences; but you can also request the values not matched, which will help you detect possible extra cleaning.\n",
    "* Pandas differentiates the **left** from the **right** data frames.\n",
    "\n",
    "At this stage, let me use two data frames to show how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo=pd.read_pickle(\"https://github.com/EvansDataScience/CTforGA_cleaning/raw/main/demoindex.pkl\")\n",
    "#and\n",
    "fragile2021=fragile[fragile.Year==2021].drop(columns=['Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragile2021.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the amount of rows of each DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.shape,fragile2021.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best scenario would be to create a merged DF with 167 rows, but the actual result is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.merge(fragile2021).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of matches among countries is far from the best scenario.\n",
    "\n",
    "Let's use set operations to find what countries are not matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OnlyDemo=set(demo.Country)-set(fragile2021.Country)\n",
    "OnlyDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OnlyFragile=set(fragile2021.Country)-set(demo.Country)\n",
    "OnlyFragile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we should try to find the what countries in _OnlyFragile_ may match the ones in _OnlyDemo_. We need to use the **fuzzy merge** approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import process as fz\n",
    "\n",
    "# look for a country in OnlyDemo and return the most similar\n",
    "[(fz.extractOne(demo, OnlyFragile),demo) for demo in sorted(OnlyDemo)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you have found some good match. As you see the ones that are wrong have  95% match or less. Let's just filter those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(fz.extractOne(demo, OnlyFragile),demo) \n",
    " for demo in sorted(OnlyDemo) \n",
    " if fz.extractOne(demo, OnlyFragile)[1]>=95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have good matches, you have to create dictionary like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changesFragile1={fz.extractOne(demo, OnlyFragile)[0]:demo \n",
    "                 for demo in sorted(OnlyDemo) \n",
    "                 if fz.extractOne(demo, OnlyFragile)[1]>=95}\n",
    "#dict of matches\n",
    "changesFragile1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use that dict for the replacements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragile2021.Country.replace(to_replace=changesFragile1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the countries in fragile2021 have more matches. \n",
    "\n",
    "This process can be done a few more times, and you can recover more rows for the merging process. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second try\n",
    "OnlyDemo=set(demo.Country)-set(fragile2021.Country)\n",
    "OnlyFragile=set(fragile2021.Country)-set(demo.Country)\n",
    "[(fz.extractOne(demo, OnlyFragile),demo) for demo in sorted(OnlyDemo)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second dict of changes\n",
    "changesFragile2={fz.extractOne(demo, OnlyFragile)[0]:demo \n",
    "                 for demo in sorted(OnlyDemo) \n",
    "                 if fz.extractOne(demo, OnlyFragile)[1]>=80}\n",
    "#dict of matches\n",
    "changesFragile2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the changes\n",
    "fragile2021.Country.replace(to_replace=changesFragile2,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third try\n",
    "OnlyDemo=set(demo.Country)-set(fragile2021.Country)\n",
    "OnlyFragile=set(fragile2021.Country)-set(demo.Country)\n",
    "[(fz.extractOne(demo, OnlyFragile),demo) for demo in sorted(OnlyDemo)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third dict of changes\n",
    "changesFragile3={fz.extractOne(demo, OnlyFragile)[0]:demo \n",
    "                 for demo in sorted(OnlyDemo) \n",
    "                 if fz.extractOne(demo, OnlyFragile)[1]>=64}\n",
    "#dict of matches\n",
    "changesFragile3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make changes\n",
    "fragile2021.Country.replace(to_replace=changesFragile3,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth try\n",
    "\n",
    "OnlyDemo=set(demo.Country)-set(fragile2021.Country)\n",
    "OnlyFragile=set(fragile2021.Country)-set(demo.Country)\n",
    "[(fz.extractOne(demo, OnlyFragile),demo) for demo in sorted(OnlyDemo)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth attempt did not offer good results. Those two countries will not be able to be matched. Let's retry the merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.merge(fragile2021).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_fragile=demo.merge(fragile2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking:\n",
    "demo_fragile.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It all look great so far. However, once you think you have the data ready, you should see the basic statistical summary of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_fragile.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A boxplot may also be helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_fragile.plot(kind='box', rot=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's pay attention to the lowest and highest values. In this case, the **Total** has a range of values different than the rest. Let's make sure all numeric columns share the same range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# prepare the process\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 10))\n",
    "\n",
    "# apply process\n",
    "arrayTotal = scaler.fit_transform(demo_fragile[['Total']])\n",
    "\n",
    "# result\n",
    "arrayTotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(arrayTotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me use that array to replace my values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_fragile['Total']=arrayTotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, these are my data values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_fragile.plot(kind='box', rot=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "\n",
    "# <font color=\"red\">Exporting file</font>\n",
    "\n",
    "The current *demo_fragile* data frame is clean and formatted. It is time to send it to a format that will keep all our work for future use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For future use in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_fragile.to_pickle(\"demo_fragile.pkl\")\n",
    "# you will need: DF=pd.read_pickle(\"demo_fragile.pkl\")\n",
    "# or:\n",
    "# from urllib.request import urlopen\n",
    "# DF=pd.read_pickle(urlopen(\"https://..../demo_fragile.pkl\"),compression=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For future  use in R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try the following before starting Python:\n",
    "#export LD_LIBRARY_PATH=\"$(python -m rpy2.situation LD_LIBRARY_PATH)\":${LD_LIBRARY_PATH}\n",
    "\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "base = importr('base')\n",
    "base.saveRDS(demo_fragile,file=\"demo_fragile.RDS\")\n",
    "\n",
    "\n",
    "#In R, you call it with: DF = readRDS(\"demo_fragile.RDS\")\n",
    "#or, if iyou read from cloud: DF = readRDS(url(\"https://..../demo_fragile.RDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
